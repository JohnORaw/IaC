# Background

In the last section of notes (Fundamentals), I argued that the field of computing is still new and immature. In the mid-1980s, the PC had only just been developed and for serious business or automation applications, mini-computers and mainframes were used. These computers were large and expensive, the decision to implement was taken with great care and normally built around a definite business case.&#x20;

For software projects I was involved with back then, the cycle from pre-sales to commissioning was typically a year and required specialist training and a deep technical knowledge. Code was written in assembly language native to the platform and was very economical; a plant was controlled entirely with a minicomputer having 128-256KB of main memory. Software was proprietary, but the code listings were shipped with the system (on paper!) and live code could be edited by field staff or indeed the customer.&#x20;

Software was modular, once the functionality of a module was confirmed by years of extended service, the building of a software load became an assembly task. Bugs were the responsibility of the supplier and were resolved expeditiously as penalties were a standard part of a project’s contracts; bugs on mature modules were rare. Software patches of any kind had a formal process associated with specification, implementation, test and documentation, but were infrequently required. The upgrading or changing of the base software was a project driven by customer requirements for functionality and might be carried out twice a decade; if the system was working, there was no reason to change hardware or software. A hacker was someone with a very good understanding of hardware and software and the term was a compliment!&#x20;

Staff training was extensive and systematic, there were mentoring and certification programs. A new engineer served a period similar to an apprenticeship. In one such company, despite being a graduate, I spent several months in overalls, working in every department on the factory floor before being deployed to the field. This was a standard methodology employed.&#x20;

The business model that supported all this was to install at a small profit, but to make substantial margin on rolling service contracts; from every perspective, it was in the supplier’s interest to have no unscheduled downtime, no equipment failures, no discernible software bugs; stability. There were sites where the customer had never experienced unscheduled downtime over a twenty year or more lifetime of the system. Scheduled maintenance was carried out on many sites bi-annually.

At this point in time, a revolution occurred in computer hardware that would be described these days as “disruptive” and is normally described as the fourth generation of computing technology. In the late 1960s, a small company called Intel were offered a contract to build a chip for a calculator (the Busicom Project) and in 1971, released what is generally accepted to be the world’s first microprocessor, a CPU on a single chip. Only the CPU was in a single package; there were still a range of support chips needed as memory and for I/O support. A 4000 based system typically used a 4001 (metal-mask programmable ROM), a 4002 (320-bit RAM) and a 4003 (10b static shift register) in addition to the 4004 CPU. The release of the 4004 4-bit processor began the microcomputer era and the fourth generation of computer hardware. The advantages of the new technology were startling. As one of the designers (Faggin) said \[1].&#x20;

“_…. nine years earlier, at about this time of the year, I had just completed another computer, made with germanium transistors, that had about the same capabilities as this one, except the new one could all fit into a single printed circuit board, instead of a few hundred boards; had about ten times higher speed; and consumed almost 1,000 times less power. What a difference nine years make_”.&#x20;

The first device designed as a general-purpose microprocessor was the 8080 c.1973. Intel intended this processor to have a ten-fold performance improvement over the 8008 and to resolve many of the problems which had emerged once the 8008 was understood. The 8080 was the earliest device the author used in large scale process automation products; a 19” rack mounted frame with a CPU board based on the 8080 and specialised measurement and I/O boards giving the capability to do control actions at 16 times per second. The 8085 c.1976 was just a repackaging of the 8080 with less support chips and some additional functionality; it was instruction set compatible with the 8080 and a straight swap-out in the process applications in which the author used them. One characteristic is clear whilst reading commentaries and analysis of the time. Each new processor release starts with a line like “designed to produce an order of magnitude increase in processing”. Over a six-year period in the 1970s, “throughput has improved one-hundredfold, the price of a CPU chip has declined from $300 to $3” \[2].&#x20;

The market leading systems which I installed in the 1980s used mini-computers and processors from 1970s, ten-year old stable hardware on a decade of operating system and software development. Suddenly, the philosophy was no longer one of stability. It was one of smaller, faster, cheaper ad infinitum, with the fastest possible delivery of new product, and a business model based on high turnover rates of equipment and short life cycles; no one was building a Model-T.&#x20;

When we look a Unix or the C programming language, we are looking back to the early 1970s.&#x20;

When we try to understand modern server hardware and operating systems, we have origins in the late 1970s and some of the work of Don Estridge’s PC group in IBM in the early 1980s.

\[1] F. Faggin, "The Making of the First Microprocessor," IEEE Solid-State Circuits Magazine, vol. 1, no. 1, pp. 8-21, 2009, doi: 10.1109/MSSC.2008.930938.&#x20;

\[2] Morse, Raveiel, Mazor, and Pohiman, "Intel Microprocessors–8008 to 8086," Computer, vol. 13, no. 10, pp.
